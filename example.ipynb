{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example usage of IDGDatasetBase\n",
    "\n",
    "IDGDatasetBase is DataLoader for image captioning.\n",
    "\n",
    "## Parameters\n",
    "\n",
    "* dataset_path\n",
    "        \n",
    "        Path to datast created by shells/pre_process.sh\n",
    "        dataset is saved to data/captions/converted/XXX/YYY.pkl as a default.\n",
    "        \n",
    "* vocab_path\n",
    "        \n",
    "        Path to vocabulary dictionary created by shells/pre_process.sh\n",
    "        vocalulary dictionary is save to data/vocab/XXX.pkl as a default.\n",
    "\n",
    "* img_root\n",
    "\n",
    "        Path to root directory that contains MSCOCO images.\n",
    "        This has to be specified when raw_caption is True.\n",
    "        \n",
    "* img_feature_root\n",
    "\n",
    "        Path to root directory that contains image features.\n",
    "        This has to be specified when raw_caption is False.\n",
    "\n",
    "* raw_caption\n",
    "\n",
    "        when raw_caption is True, it returns list of tokenized captions.\n",
    "        if False, it returns numpy.nparray.\n",
    "\n",
    "* raw_img\n",
    "\n",
    "        When raw_img is True, it use raw images downloaded from MSCOCO dataset.\n",
    "        if False, it uses image features processed beforehand.\n",
    "        but this repository doesn't contain preprocessed image features.\n",
    "        So it can't be False.\n",
    "\n",
    "* img_mean\n",
    "\n",
    "        This parameter is used for preprocess images.\n",
    "        The default value is \"imagenet\", mean value of imagenet is substracted from each images.\n",
    "        if None, original RGB values are used.\n",
    "        You can specify mean values like (123.44, 355.22, 235.2)\n",
    "\n",
    "* img_size\n",
    "        \n",
    "        Output size of image. Default size is (244, 244).\n",
    "        if image size is more/less than img_size, the image is automatically resized.\n",
    "\n",
    "* preload_features\n",
    "\n",
    "        When preload_features is True, all features preprocessed beforehand are preloaded onto RAM.\n",
    "        it consumes much RAM.\n",
    "        if dataset is MSCOCO train 2014 and each feature size is 2048, then it would takes about &GB.\n",
    "        But this parameter is also can't be used because This repository doesn't contain preprocessed\n",
    "        image features. \n",
    "        If you want to use preprocessed image features. you use ResNet or something to get them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainer\n",
    "from IDGDataset import IDGDatasetBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set paths\n",
    "# if you use shells/pre_process_sh, dataset contains both image paths and captions, \n",
    "# and vocabulary dictionary is saved to designated directory like below.\n",
    "\n",
    "dataset_path = \"data/captions/converted/MSCOCO_captions/train2014.pkl\"\n",
    "vocab_path = \"data/vocab/mscoco_train2014_vocab.pkl\"\n",
    "img_root = \"data/images/original\"\n",
    "#img_feature_root = \"data/images/ResNet50\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set other configurations\n",
    "# see help(IDGDatasetBase) for detail.\n",
    "raw_caption = False\n",
    "raw_img = True\n",
    "img_mean = \"imagenet\"\n",
    "img_size = (224,224)\n",
    "preload_features = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset using chainer.dataset.Mixin wrapper.\n",
    "train_data = IDGDatasetBase(\n",
    "                dataset_path,\n",
    "                vocab_path,\n",
    "                img_root=img_root,\n",
    "                raw_caption=raw_caption,\n",
    "                raw_img=raw_img,\n",
    "                img_mean=img_mean,\n",
    "                img_size=img_size,\n",
    "                preload_features=preload_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ -85.255325    -81.398186    -73.19411    ...  -14.928864\n",
      "     -3.4185715    35.591484  ]\n",
      "  [ -82.90839     -93.30635     -76.30635    ...   -9.265175\n",
      "      7.305847     47.3059    ]\n",
      "  [ -83.29614     -70.13288     -89.00023    ...   -6.489624\n",
      "     -3.969635     -7.7549973 ]\n",
      "  ...\n",
      "  [ -89.29612     -89.04106     -92.7349     ...  146.63242\n",
      "    146.6528      145.20386   ]\n",
      "  [ -84.09198     -84.88788     -84.17357    ...  146.81613\n",
      "    145.63248     146.13248   ]\n",
      "  [ -89.22446     -77.29615     -88.765656   ...  148.16312\n",
      "    142.32632     142.47937   ]]\n",
      "\n",
      " [[ -78.58512     -80.09532     -63.83002    ...  -30.493385\n",
      "    -19.513618     36.353554  ]\n",
      "  [ -83.33002     -83.360634    -79.08512    ...  -24.074593\n",
      "     -7.656563     46.28225   ]\n",
      "  [ -77.09532     -76.47288     -77.860634   ...  -11.809265\n",
      "    -15.411659    -13.941849  ]\n",
      "  ...\n",
      "  [-100.717766    -98.31984     -97.72798    ...   51.363853\n",
      "     48.75158      46.221     ]\n",
      "  [ -98.197395   -100.156525    -92.411606   ...   48.11904\n",
      "     45.792473     45.292473  ]\n",
      "  [ -80.3909      -76.564606    -82.60553    ...   46.465996\n",
      "     42.486305     41.567955  ]]\n",
      "\n",
      " [[ -48.87388     -51.37388     -40.118767   ...   -1.5981369\n",
      "      3.5139465    74.45217   ]\n",
      "  [ -43.853462    -53.46572     -33.312645   ...   -0.26104736\n",
      "     15.1669235    81.70742   ]\n",
      "  [ -44.833054    -46.445297    -38.935097   ...   15.71862\n",
      "      4.2791214    -1.4655991 ]\n",
      "  ...\n",
      "  [ -79.455505    -80.618805    -85.27184    ...   33.605705\n",
      "     33.78936      27.605705  ]\n",
      "  [ -73.60858     -79.404465    -76.69018    ...   32.993553\n",
      "     28.891472     31.677177  ]\n",
      "  [ -58.934776    -55.179924    -63.588203   ...   32.564995\n",
      "     25.585304     28.809776  ]]]\n",
      "[   1    3  248  340   23    8    0 4069  164  285    2]\n"
     ]
    }
   ],
   "source": [
    "# preprocessed image and encoded caption can be loaded.\n",
    "img, caption = train_data[1]\n",
    "\n",
    "print(img)\n",
    "print(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoded caption\n",
      "['<SOS>', 'a', 'long', 'restaurant', 'table', 'with', '<UNK>', 'rounded', 'back', 'chairs', '<EOS>']\n",
      "encoded caption\n",
      "[1, 3, 248, 340, 23, 8, 0, 4069, 164, 285, 2]\n"
     ]
    }
   ],
   "source": [
    "# index2token decode captions encoded.\n",
    "# token2index encode tokenized captions.\n",
    "\n",
    "img, caption = train_data[1]\n",
    "\n",
    "\n",
    "dec_caption = train_data.index2token(caption)\n",
    "\n",
    "print('decoded caption')\n",
    "print(dec_caption)\n",
    "\n",
    "re_encoded_caption = train_data.token2index(dec_caption)\n",
    "\n",
    "print('encoded caption')\n",
    "print(re_encoded_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can get list of words by get_word_ids.\n",
    "word_ids = train_data.get_word_ids\n",
    "\n",
    "#print(word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words: 8823\n"
     ]
    }
   ],
   "source": [
    "# number of words\n",
    "num_words = len(word_ids)\n",
    "\n",
    "print('The number of words: %d' % num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unk ratio: 0.061\n"
     ]
    }
   ],
   "source": [
    "# you can get <UNK> ratio by get_unk_rato\n",
    "unk_ratio = train_data.get_unk_ratio\n",
    "\n",
    "print('unk ratio: %.3f' %unk_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you just need to send train_data to chainer.iterators.SerialIterators()\n",
    "batchsize = 128\n",
    "train_iter = chainer.iterators.SerialIterator(train_data, batchsize)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
